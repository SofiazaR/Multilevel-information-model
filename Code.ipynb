{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fG7zxppFbGkh",
        "bITT2agTnrKl"
      ],
      "authorship_tag": "ABX9TyPOsQGdhE+saGaWozQpKj06",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SofiazaR/Multilevel-information-model/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1"
      ],
      "metadata": {
        "id": "GAvLVgiEZ5ON"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTOKGXTXZ4u6",
        "outputId": "020602a9-e542-456c-b3e3-4e025ddcebfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "Ошибка при запросе на сайт: /\n",
            "Ошибка при запросе на сайт: #\n",
            "Ошибка при запросе на сайт: /categories\n",
            "Ошибка при запросе на сайт: /tag\n",
            "Ошибка при запросе на сайт: /collections\n",
            "Ошибка при запросе на сайт: /samples/new\n",
            "Ошибка при запросе на сайт: /about\n",
            "0\n",
            "Ошибка при запросе на сайт: /users/sign_in\n",
            "Ошибка при запросе на сайт: /users/sign_up\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "Ошибка при запросе на сайт: /privacy\n",
            "Ошибка при запросе на сайт: /cdn-cgi/l/email-protection#d1b9b4bdbdbe91a2b0bca1bdb4b7beb2a4a2ffb2bebc\n",
            "0\n",
            "0\n",
            "Ошибка при запросе на сайт: #/portal/signin\n",
            "Ошибка при запросе на сайт: #/portal/signup\n",
            "Ошибка при запросе на сайт: /blog/essential-tips-ableton-live-11/\n",
            "Ошибка при запросе на сайт: /blog/the-secret-to-perfect-transitions/\n",
            "Ошибка при запросе на сайт: /blog/rave-hits-pack/\n",
            "Ошибка при запросе на сайт: /blog/bootleg-remixing/\n",
            "Ошибка при запросе на сайт: /blog/ableton-live-shortcut-list/\n",
            "Ошибка при запросе на сайт: /blog/drum-machine-magic-create-beats-drum-patterns-that-stick/\n",
            "Ошибка при запросе на сайт: /blog/artist-spotlight-danilo-stellet/\n",
            "Ошибка при запросе на сайт: /blog/make-idm-with-samples/\n",
            "Ошибка при запросе на сайт: /blog/microsampling-in-music/\n",
            "Ошибка при запросе на сайт: /blog/manipulating-vocal-samples-techniques/\n",
            "Ошибка при запросе на сайт: /blog/lo-fi-production-perspective/\n",
            "Ошибка при запросе на сайт: /blog/stop-overthinking-finish-track/\n",
            "Ошибка при запросе на сайт: /blog/acid-breaks-production/\n",
            "Ошибка при запросе на сайт: /blog/synth-settings-top-40-hits/\n",
            "Ошибка при запросе на сайт: /blog/music-making-limited-resources/\n",
            "Ошибка при запросе на сайт: /blog/mixing-tips-beginners/\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "5\n",
            "Ошибка при запросе на сайт: /t/contact_us/\n",
            "6\n",
            "7\n",
            "7\n",
            "Ошибка при запросе на сайт: /t/terms\n",
            "Ошибка при запросе на сайт: /t/privacy\n",
            "8\n",
            "9\n",
            "Ошибка при запросе на сайт: /new\n",
            "10\n",
            "Ошибка при запросе на сайт: /tag/calm\n",
            "Ошибка при запросе на сайт: /tag/clean\n",
            "Ошибка при запросе на сайт: /tag/confident\n",
            "Ошибка при запросе на сайт: /tag/cool\n",
            "Ошибка при запросе на сайт: /tag/dynamic\n",
            "Ошибка при запросе на сайт: /tag/echoing\n",
            "Ошибка при запросе на сайт: /tag/ethereal\n",
            "Ошибка при запросе на сайт: /users/yo-mama-9a67922e-34df-476b-ba69-6ff295412663\n",
            "Ошибка при запросе на сайт: /samples/times-feedback-vocal-singing/download\n",
            "Ошибка при запросе на сайт: /tag/ambient\n",
            "Ошибка при запросе на сайт: /tag/calming\n",
            "Ошибка при запросе на сайт: /tag/chilled\n",
            "Ошибка при запросе на сайт: /tag/chillout\n",
            "Ошибка при запросе на сайт: /tag/delicate\n",
            "Ошибка при запросе на сайт: /tag/dreamy\n",
            "Ошибка при запросе на сайт: /users/andryk-sanchez\n",
            "Ошибка при запросе на сайт: /samples/keys-bells-delicate-loop-soft-melody/download\n",
            "Ошибка при запросе на сайт: /tag/chord\n",
            "Ошибка при запросе на сайт: /tag/classical\n",
            "Ошибка при запросе на сайт: /tag/elegant\n",
            "Ошибка при запросе на сайт: /users/elrali-skalli-2f42ccce-a6fa-4e2a-80b3-c5059fd333c6\n",
            "Ошибка при запросе на сайт: /samples/clean-round-melody-hammong-organ/download\n",
            "Ошибка при запросе на сайт: /tag/aggressive\n",
            "Ошибка при запросе на сайт: /tag/bassy\n",
            "Ошибка при запросе на сайт: /tag/bouncy\n",
            "Ошибка при запросе на сайт: /tag/dark\n",
            "Ошибка при запросе на сайт: /tag/digital\n",
            "Ошибка при запросе на сайт: /users/fuyri-deadxl\n",
            "Ошибка при запросе на сайт: /samples/hard-bass-beat-trap-style-drums/download\n",
            "Ошибка при запросе на сайт: /tag/climactic\n",
            "Ошибка при запросе на сайт: /tag/deturned\n",
            "Ошибка при запросе на сайт: /tag/drums\n",
            "Ошибка при запросе на сайт: /tag/full\n",
            "Ошибка при запросе на сайт: /tag/funk-soul\n",
            "Ошибка при запросе на сайт: /tag/legato\n",
            "Ошибка при запросе на сайт: /users/cazi-vitti-officiel\n",
            "Ошибка при запросе на сайт: /samples/rare-soul-loop-melancholic-climatic-melody/download\n",
            "Ошибка при запросе на сайт: /tag/acoustic\n",
            "Ошибка при запросе на сайт: /tag/arpeggiated\n",
            "Ошибка при запросе на сайт: /tag/exotic\n",
            "Ошибка при запросе на сайт: /users/user2423007272940\n",
            "Ошибка при запросе на сайт: /samples/mandolin-arpeggio-loop-fast-progression/download\n",
            "Ошибка при запросе на сайт: /tag/carefree\n",
            "Ошибка при запросе на сайт: /tag/high\n",
            "Ошибка при запросе на сайт: /users/enwin\n",
            "Ошибка при запросе на сайт: /samples/rap-plucked-instrumental-loop-carefree-exotic-melody/download\n",
            "Ошибка при запросе на сайт: /tag/lo-fi-f35cc8b9-4291-43c4-a50d-a60eaebbd751\n",
            "Ошибка при запросе на сайт: /samples/plucks-high-notes-clean-loop/download\n",
            "10\n",
            "10\n",
            "10\n",
            "10\n",
            "Ошибка при запросе на сайт: /tag/closed-hi-hats\n",
            "Ошибка при запросе на сайт: /tag/hip-hop\n",
            "Ошибка при запросе на сайт: /tag/monophonic\n",
            "Ошибка при запросе на сайт: /tag/one-shot\n",
            "Ошибка при запросе на сайт: /tag/processed\n",
            "Ошибка при запросе на сайт: /tag/trap\n",
            "Ошибка при запросе на сайт: /users/scooter\n",
            "Ошибка при запросе на сайт: /samples/lo-fi-trap-hi-hat/download\n",
            "Ошибка при запросе на сайт: /tag/bass\n",
            "Ошибка при запросе на сайт: /tag/boomy\n",
            "Ошибка при запросе на сайт: /tag/digital-synth-bass\n",
            "Ошибка при запросе на сайт: /tag/dry\n",
            "Ошибка при запросе на сайт: /users/bapesta-whiteboy\n",
            "Ошибка при запросе на сайт: /samples/duwap-kaine-808-bass/download\n",
            "Ошибка при запросе на сайт: /tag/acoustic-guitar\n",
            "Ошибка при запросе на сайт: /tag/bright\n",
            "Ошибка при запросе на сайт: /users/72d6350a-9bbd-4c86-b161-4054e94ee2e1\n",
            "Ошибка при запросе на сайт: /samples/mexican-guitar-type-loop-gunna-type-melody/download\n",
            "Ошибка при запросе на сайт: /tag/ethnic-world\n",
            "Ошибка при запросе на сайт: /users/fda4a69e-1370-4a75-a10d-260b02fb0889\n",
            "Ошибка при запросе на сайт: /samples/weird-plucked-loop-japanese-trap-koto/download\n",
            "Ошибка при запросе на сайт: /tag/compressed\n",
            "Ошибка при запросе на сайт: /users/i-i-28ea7977-5b06-48ad-9532-6c655b023ec7\n",
            "Ошибка при запросе на сайт: /samples/synth-plucked-chilled-ambient-loop/download\n",
            "Ошибка при запросе на сайт: /tag/angry\n",
            "Ошибка при запросе на сайт: /tag/coarse-harsh\n",
            "Ошибка при запросе на сайт: /tag/drum-bass\n",
            "Ошибка при запросе на сайт: /users/ralph-peace\n",
            "Ошибка при запросе на сайт: /samples/build-up-snares-rising-loop-drums/download\n",
            "Ошибка при запросе на сайт: /tag/200bpm\n",
            "Ошибка при запросе на сайт: /users/amae\n",
            "Ошибка при запросе на сайт: /samples/frenchcore-type-loop-crazy-speed-kicks/download\n",
            "Ошибка при запросе на сайт: /tag/buzzy\n",
            "Ошибка при запросе на сайт: /tag/dubstep\n",
            "Ошибка при запросе на сайт: /users/micah-dunsmoor\n",
            "Ошибка при запросе на сайт: /samples/positive-pitched-vocal-loop-with-buzzy-hits/download\n",
            "Ошибка при запросе на сайт: None\n",
            "Ошибка при запросе на сайт: /cdn-cgi/l/email-protection#dcb4b9b0b0b39cafbdb1acb0b9bab3bfa9aff2bfb3b1\n",
            "Ошибка при запросе на сайт: /blog/french-house-explored/\n",
            "Ошибка при запросе на сайт: /blog/how-to-make-a-hip-hop-beat-using-only-5-samples/\n",
            "Ошибка при запросе на сайт: /blog/trap-samples-making/\n",
            "Ошибка при запросе на сайт: /blog/samples-techniques-logic-pro/\n",
            "Ошибка при запросе на сайт: /blog/tune-samples-ableton/\n",
            "Ошибка при запросе на сайт: /blog/afro-beat-2024/\n",
            "Ошибка при запросе на сайт: /blog/ableton-live-effect-rack/\n",
            "Ошибка при запросе на сайт: /blog/drum-programming-guide/\n",
            "Ошибка при запросе на сайт: /blog/flstudioableton-daw/\n",
            "Ошибка при запросе на сайт: /blog/make-anything-sound-psychedelic/\n",
            "Ошибка при запросе на сайт: /blog/make-robotic-vocal-effects/\n",
            "Ошибка при запросе на сайт: /blog/inside-electro-sample-pack/\n",
            "Ошибка при запросе на сайт: /blog/reliable-music-samples/\n",
            "Ошибка при запросе на сайт: /blog/pierre-bourne-perc-hits-trap/\n",
            "Ошибка при запросе на сайт: /blog/make-samples-your-own/\n",
            "Ошибка при запросе на сайт: /blog/what-is-granular-sampling/\n",
            "Ошибка при запросе на сайт: /creators/\n",
            "Ошибка при запросе на сайт: /publishers/\n",
            "Ошибка при запросе на сайт: /business/\n",
            "Ошибка при запросе на сайт: /docs/\n",
            "Ошибка при запросе на сайт: /explore/\n",
            "Ошибка при запросе на сайт: /marketplace/\n",
            "Ошибка при запросе на сайт: /resources/\n",
            "Ошибка при запросе на сайт: /themes/\n",
            "Ошибка при запросе на сайт: /help/\n",
            "Ошибка при запросе на сайт: /integrations/\n",
            "Ошибка при запросе на сайт: /changelog/\n",
            "Ошибка при запросе на сайт: /experts/\n",
            "Ошибка при запросе на сайт: /about/\n",
            "Ошибка при запросе на сайт: /pricing/\n",
            "Ошибка при запросе на сайт: javascript:;\n",
            "10\n",
            "10\n",
            "Ошибка при запросе на сайт: /help/using-custom-domains/\n",
            "Ошибка при запросе на сайт: /docs/install/\n",
            "Ошибка при запросе на сайт: /help/how-do-i-reset-my-password/\n",
            "Ошибка при запросе на сайт: /help/cloudflare-domain-setup/\n",
            "Ошибка при запросе на сайт: /docs/themes/\n",
            "10\n",
            "Ошибка при запросе на сайт: /integrations/geckoboard/\n",
            "Ошибка при запросе на сайт: /integrations/discourse/\n",
            "Ошибка при запросе на сайт: /integrations/imgur/\n",
            "Ошибка при запросе на сайт: /integrations/mailchimp/\n",
            "Ошибка при запросе на сайт: /integrations/pinterest/\n",
            "Ошибка при запросе на сайт: /integrations/helpscout/\n",
            "Ошибка при запросе на сайт: /integrations/buffer/\n",
            "Ошибка при запросе на сайт: /integrations/unsplash/\n",
            "Ошибка при запросе на сайт: /integrations/airtable/\n",
            "Ошибка при запросе на сайт: /integrations/vimeo/\n",
            "Ошибка при запросе на сайт: /integrations/discord/\n",
            "Ошибка при запросе на сайт: /integrations/spotify/\n",
            "Ошибка при запросе на сайт: /integrations/ulysses/\n",
            "Ошибка при запросе на сайт: /integrations/activecampaign/\n",
            "Ошибка при запросе на сайт: /integrations/twitter/\n",
            "Ошибка при запросе на сайт: /integrations/zapier/\n",
            "Ошибка при запросе на сайт: /integrations/stripe/\n",
            "Ошибка при запросе на сайт: /integrations/slack/\n",
            "Ошибка при запросе на сайт: /integrations/instagram/\n",
            "Ошибка при запросе на сайт: /integrations/facebook/\n",
            "Ошибка при запросе на сайт: /integrations/ia-writer/\n",
            "Ошибка при запросе на сайт: /integrations/youtube/\n",
            "Ошибка при запросе на сайт: /integrations/feedly/\n",
            "Ошибка при запросе на сайт: /integrations/analytics/\n",
            "Ошибка при запросе на сайт: /integrations/github/\n",
            "Ошибка при запросе на сайт: /integrations/netlify/\n",
            "Ошибка при запросе на сайт: /integrations/lets-encrypt/\n",
            "Ошибка при запросе на сайт: /integrations/plausible/\n",
            "Ошибка при запросе на сайт: /integrations/chartmogul/\n",
            "Ошибка при запросе на сайт: /integrations/cove-comments/\n",
            "Ошибка при запросе на сайт: /integrations/udesly/\n",
            "Ошибка при запросе на сайт: /integrations/giphy/\n",
            "Ошибка при запросе на сайт: /integrations/intercom/\n",
            "Ошибка при запросе на сайт: /integrations/flickr/\n",
            "Ошибка при запросе на сайт: /concierge/\n",
            "11\n",
            "11\n",
            "Ошибка при запросе на сайт: /feature-index/\n",
            "Ошибка при запросе на сайт: /docs/content-api/\n",
            "Ошибка при запросе на сайт: /docs/security/\n",
            "11\n",
            "Ошибка при запросе на сайт: /tutorials/\n",
            "11\n",
            "11\n",
            "Ошибка при запросе на сайт: /vs/substack/\n",
            "Ошибка при запросе на сайт: /vs/wordpress/\n",
            "Ошибка при запросе на сайт: /vs/medium/\n",
            "Ошибка при запросе на сайт: /vs/memberful/\n",
            "Ошибка при запросе на сайт: /vs/patreon/\n",
            "Ошибка при запросе на сайт: /alternatives/\n",
            "11\n",
            "11\n",
            "11\n",
            "11\n",
            "Ошибка при запросе на сайт: /terms/\n",
            "Ошибка при запросе на сайт: /privacy/\n",
            "Ошибка при запросе на сайт: /contact/\n",
            "12\n",
            "Ошибка при запросе на сайт: #twtr-main\n",
            "12\n",
            "12\n",
            "13\n",
            "14\n",
            "14\n",
            "14\n",
            "Ошибка при запросе на сайт: https://twitter3e4tixl4xyajtrzo62zg5vztmjuricljdp2c5kshju4avyoid.onion\n",
            "Ошибка при запросе на сайт: twitterhbmit57bzbcjnujedrn7uk73geo4ackio4lxdj6t7w6f4zsid.onion\n",
            "Ошибка при запросе на сайт: twitterhpgjerufcvrmzerg2novpipy42rk3anvb5b7np4zggm4rwaqd.onion\n",
            "14\n",
            "Ошибка при запросе на сайт: https://status.twitterstat.us/\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "Ошибка при запросе на сайт: /en/forms.html\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "16\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "19\n",
            "20\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "22\n",
            "23\n",
            "23\n",
            "24\n",
            "24\n",
            "Ошибка при запросе на сайт: #update-Chapter1\n",
            "Ошибка при запросе на сайт: #update-Chapter2\n",
            "Ошибка при запросе на сайт: #update-Chapter3\n",
            "Ошибка при запросе на сайт: #update-Chapter4\n",
            "Ошибка при запросе на сайт: #update-Chapter5\n",
            "Ошибка при запросе на сайт: #update-Chapter6\n",
            "25\n",
            "26\n",
            "26\n",
            "26\n",
            "Ошибка при запросе на сайт: mailto:copyright@x.com\n",
            "27\n",
            "28\n",
            "28\n",
            "28\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "31\n",
            "31\n",
            "31\n",
            "32\n",
            "33\n",
            "33\n",
            "33\n",
            "33\n",
            "Ошибка при запросе на сайт: #intTerms-update-Chapter1\n",
            "Ошибка при запросе на сайт: #intTerms-update-Chapter2\n",
            "Ошибка при запросе на сайт: #intTerms-update-Chapter3\n",
            "Ошибка при запросе на сайт: #intTerms-update-Chapter4\n",
            "Ошибка при запросе на сайт: #intTerms-update-Chapter5\n",
            "Ошибка при запросе на сайт: #intTerms-update-Chapter6\n",
            "34\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "Ошибка при запросе на сайт: #x-privacy-3.1\n",
            "Ошибка при запросе на сайт: #x-privacy-1\n",
            "70\n",
            "Ошибка при запросе на сайт: #x-privacy-2\n",
            "70\n",
            "Ошибка при запросе на сайт: #x-privacy-9.1\n",
            "71\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "75\n",
            "75\n",
            "75\n",
            "75\n",
            "75\n",
            "75\n",
            "75\n",
            "76\n",
            "76\n",
            "77\n",
            "77\n",
            "78\n",
            "78\n",
            "78\n",
            "78\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "82\n",
            "83\n",
            "83\n",
            "83\n",
            "83\n",
            "83\n",
            "83\n",
            "83\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "Finished\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "sites = ['https://samplefocus.com/tag/memphis','https://memphis.dev/']\n",
        "\n",
        "links = []\n",
        "for i in sites:\n",
        "    if not i in links:\n",
        "        links.append(i)\n",
        "\n",
        "count = 0\n",
        "i = 0\n",
        "\n",
        "if not os.path.exists(fr'texts'):\n",
        "    os.makedirs(fr'texts')\n",
        "\n",
        "while count < 100:\n",
        "    if not links[i] in links:\n",
        "        i += 1\n",
        "        continue\n",
        "    current_site = links[i]\n",
        "    i += 1\n",
        "\n",
        "    try:\n",
        "        r = requests.get(current_site)\n",
        "    except:\n",
        "        print(f'Ошибка при запросе на сайт: {current_site}')\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(r.text, 'lxml')\n",
        "    text = soup.get_text()\n",
        "\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    num_words = len(text.split())\n",
        "\n",
        "    all_a = soup.find_all('a')\n",
        "\n",
        "    for a in all_a:\n",
        "        if not links.__contains__(a.get('href')):\n",
        "            links.append(a.get('href'))\n",
        "    if num_words >= 1000:\n",
        "        count += 1\n",
        "        with open(f\"texts/{count}.txt\", \"w\", encoding=\"utf-8\") as myfile:\n",
        "            myfile.write(text)\n",
        "            myfile.close()\n",
        "        with open(fr\"index.txt\", \"a\") as myfile:\n",
        "            myfile.write(f\"{count}.txt -> {current_site}\\n\")\n",
        "    print(count)\n",
        "\n",
        "print(\"Finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2"
      ],
      "metadata": {
        "id": "jpH9EmQxa1Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjGUPWrchXqO",
        "outputId": "4205373d-59f0-41a6-c404-81f2eedf6108"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3\n",
            "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.1 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3  # библиотека, которая может взять начальную форму слов\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import string\n",
        "import unicodedata\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "# CONJ- союз\n",
        "# INTJ- междометие\n",
        "functors_pos = {'INTJ', 'PRCL', 'CONJ', 'PREP'}  # function words - предлоги и незначащие слова\n",
        "path_of_files = \"texts/\"\n",
        "\n",
        "\n",
        "def pos(word, morth=pymorphy3.MorphAnalyzer()):\n",
        "    \"Return a likely part of speech for the *word*.\"\"\"\n",
        "    return morth.parse(word)[0].tag.POS\n",
        "\n",
        "\n",
        "def letters(text):\n",
        "    return ''.join(filter(str.isalpha, text))\n",
        "\n",
        "\n",
        "def lemmatize(words, tokenized_map):\n",
        "    for word in words:\n",
        "        if pos(word) not in functors_pos:\n",
        "            p = morph.parse(word)[0].normal_form\n",
        "            arr = tokenized_map.get(p)\n",
        "            wnl = WordNetLemmatizer()\n",
        "            if arr is None:\n",
        "                new_arr = set()\n",
        "                new_arr.add(word)\n",
        "                tokenized_map[p] = new_arr\n",
        "            else:\n",
        "                arr.add(word)\n",
        "\n",
        "\n",
        "\n",
        "def get_clear_words(text): # Очистка слов\n",
        "    spaced_text = \"\".join([c if c.isalpha() else ' ' for c in text])\n",
        "    words = spaced_text.split()\n",
        "    clear_words = []\n",
        "    for w in words:\n",
        "      if len(w) > 3:\n",
        "        w = w.encode('ASCII', errors='ignore').decode('ASCII')\n",
        "        w = bytes(w, 'ASCII').decode('ASCII', 'ignore')\n",
        "        clear_words.append(letters(w))\n",
        "    return clear_words\n",
        "\n",
        "\n",
        "def read_text_from_file(file): # функция для открытия файлов\n",
        "    with io.open(file, mode='r', encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def get_files_from_path(path):# функция для получения файлов\n",
        "    arr = os.listdir(path)\n",
        "    return arr\n",
        "\n",
        "files = get_files_from_path(path_of_files)\n",
        "for f in files: # проходимся по всем файлам и для каждого создаем список лемм и токенов\n",
        "    print(f)\n",
        "    if f > '0':\n",
        "        text = read_text_from_file(path_of_files + f)\n",
        "        words = get_clear_words(text)\n",
        "        tokenized_map = {}\n",
        "        lemmatize(words, tokenized_map)\n",
        "\n",
        "        all_tokens = tokenized_map.values()\n",
        "        with io.open('tokens/tokens_' + str(f), 'w', encoding=\"utf-8\") as ff:\n",
        "            all_tokens =  [item for sublist in all_tokens for item in sublist]\n",
        "            all_tokens.sort()\n",
        "            for t in all_tokens:\n",
        "               ff.write(f\"{t}\\n\")\n",
        "\n",
        "        with io.open('lemmas/lemmas_' + str(f), 'w', encoding=\"utf-8\") as ff:\n",
        "            items = tokenized_map.items()\n",
        "            items = sorted(items)\n",
        "            for lemma in items:\n",
        "                ff.write(f\"{lemma}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH9bBeHea4MA",
        "outputId": "3191b33c-4bcb-48c2-e8f7-6172241822ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51.txt\n",
            "34.txt\n",
            "26.txt\n",
            "76.txt\n",
            "61.txt\n",
            "15.txt\n",
            "57.txt\n",
            "41.txt\n",
            "81.txt\n",
            "29.txt\n",
            "47.txt\n",
            "87.txt\n",
            "35.txt\n",
            "72.txt\n",
            "52.txt\n",
            "65.txt\n",
            "58.txt\n",
            "24.txt\n",
            "73.txt\n",
            "48.txt\n",
            "12.txt\n",
            "5.txt\n",
            "90.txt\n",
            "30.txt\n",
            "89.txt\n",
            "18.txt\n",
            "93.txt\n",
            "46.txt\n",
            "95.txt\n",
            "94.txt\n",
            "84.txt\n",
            "66.txt\n",
            "97.txt\n",
            "44.txt\n",
            "50.txt\n",
            "74.txt\n",
            "27.txt\n",
            "14.txt\n",
            "78.txt\n",
            "9.txt\n",
            "64.txt\n",
            "54.txt\n",
            "25.txt\n",
            "96.txt\n",
            "1.txt\n",
            "36.txt\n",
            "88.txt\n",
            "79.txt\n",
            "33.txt\n",
            "63.txt\n",
            "39.txt\n",
            "100.txt\n",
            "10.txt\n",
            "32.txt\n",
            "70.txt\n",
            "21.txt\n",
            "13.txt\n",
            "82.txt\n",
            "42.txt\n",
            "4.txt\n",
            "49.txt\n",
            "55.txt\n",
            "6.txt\n",
            "86.txt\n",
            "17.txt\n",
            "77.txt\n",
            "80.txt\n",
            "68.txt\n",
            "45.txt\n",
            "56.txt\n",
            "69.txt\n",
            "31.txt\n",
            "43.txt\n",
            "11.txt\n",
            "22.txt\n",
            "62.txt\n",
            "38.txt\n",
            "59.txt\n",
            "92.txt\n",
            "2.txt\n",
            "40.txt\n",
            "8.txt\n",
            "75.txt\n",
            "28.txt\n",
            "16.txt\n",
            "19.txt\n",
            "83.txt\n",
            "7.txt\n",
            "60.txt\n",
            "67.txt\n",
            "23.txt\n",
            "37.txt\n",
            "3.txt\n",
            "91.txt\n",
            "53.txt\n",
            "85.txt\n",
            "98.txt\n",
            "71.txt\n",
            "20.txt\n",
            "99.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3"
      ],
      "metadata": {
        "id": "fG7zxppFbGkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json as j\n",
        "\n",
        "json = dict()\n",
        "\n",
        "directory = os.fsencode(r'texts')\n",
        "count = 0\n",
        "for file in os.listdir(directory):\n",
        "    file_num = file.decode(\"ASCII\").split('.')[0]\n",
        "\n",
        "    if file_num > '0':\n",
        "        with open(rf'texts/{file_num}.txt', 'r', encoding='utf-8') as file:\n",
        "            data = file.read()\n",
        "\n",
        "    for word in data.split(' '):\n",
        "        if word in json:\n",
        "            if not json.get(word).__contains__(file_num):\n",
        "                json.get(word).append(file_num)\n",
        "        else:\n",
        "            json[word] = [file_num]\n",
        "\n",
        "with open(rf'index2.json', 'w', encoding='utf-8') as file:\n",
        "    j.dump(json, file, ensure_ascii=False)\n",
        "\n",
        "print(len(json.keys()))\n",
        "print('Success')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZt49YtebJM-",
        "outputId": "2c5bdd32-5bfd-44a5-f848-901487d676d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77919\n",
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import sys\n",
        "\n",
        "a = input()\n",
        "\n",
        "arr = a.split(' ')\n",
        "\n",
        "if len(arr) != 5:\n",
        "    sys.exit('Некорректное выражение')\n",
        "\n",
        "with open(r'index2.json', 'r',\n",
        "          encoding='utf-8') as file:\n",
        "    index = ast.literal_eval(file.read())\n",
        "\n",
        "all_values = set(range(1, 101))\n",
        "all_values = set([str(i) for i in all_values])\n",
        "\n",
        "new_arr = []\n",
        "\n",
        "for i in range(len(arr)):\n",
        "    if i % 2 == 0:\n",
        "        print(arr[i])\n",
        "        if arr[i].startswith('NOT'):\n",
        "            if index.get(arr[i][1:]) is None:\n",
        "                sys.exit('Слова из выражения не существуют')\n",
        "            new_arr.append(all_values.difference(index.get(arr[i][1:])))\n",
        "            arr[i] = arr[i][1:]\n",
        "        else:\n",
        "            if index.get(arr[i]) is None:\n",
        "                sys.exit('Слова из выражения не существуют')\n",
        "            new_arr.append(index.get(arr[i]))\n",
        "\n",
        "if arr[1] in {'OR'}:\n",
        "    if len(new_arr) < 3:\n",
        "        sys.exit('Некорректное выражение')\n",
        "    if arr[3] in {'OR'}:\n",
        "        result = set(new_arr[0]) & set(new_arr[1]) | set(new_arr[2])\n",
        "    elif arr[3] in {'AND'}:\n",
        "        result = set(new_arr[0]) & set(new_arr[1]) | set(new_arr[2])\n",
        "elif arr[1] in {'AND'}:\n",
        "    if len(new_arr) < 3:\n",
        "        sys.exit('Некорректное выражение')\n",
        "    if arr[3] in {'OR'}:\n",
        "        result = set(new_arr[0]) & set(new_arr[1]) | set(new_arr[2])\n",
        "    elif arr[3] in {'AND'}:\n",
        "        result = set(new_arr[0]) & set(new_arr[1]) | set(new_arr[2])\n",
        "\n",
        "if len(result) == 0:\n",
        "    print(\"не найдено\")\n",
        "else:\n",
        "    result = set([int(i) for i in result])\n",
        "    print(sorted(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9V7kmpiAiVCO",
        "outputId": "6d3a3951-0df5-4326-c0f0-35f423149457"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-69cb7207e552>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка файлов"
      ],
      "metadata": {
        "id": "bITT2agTnrKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/texts.zip /content/texts\n",
        "!zip -r /content/tokens.zip /content/tokens\n",
        "!zip -r /content/lemmas.zip /content/lemmas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4HZfmdWlFnv",
        "outputId": "4156880d-3f00-4c05-863c-b1414784daca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/texts/ (stored 0%)\n",
            "  adding: content/texts/51.txt (deflated 80%)\n",
            "  adding: content/texts/34.txt (deflated 64%)\n",
            "  adding: content/texts/26.txt (deflated 64%)\n",
            "  adding: content/texts/76.txt (deflated 68%)\n",
            "  adding: content/texts/61.txt (deflated 79%)\n",
            "  adding: content/texts/15.txt (deflated 54%)\n",
            "  adding: content/texts/57.txt (deflated 78%)\n",
            "  adding: content/texts/41.txt (deflated 81%)\n",
            "  adding: content/texts/81.txt (deflated 69%)\n",
            "  adding: content/texts/29.txt (deflated 62%)\n",
            "  adding: content/texts/47.txt (deflated 78%)\n",
            "  adding: content/texts/87.txt (deflated 71%)\n",
            "  adding: content/texts/35.txt (deflated 77%)\n",
            "  adding: content/texts/72.txt (deflated 60%)\n",
            "  adding: content/texts/52.txt (deflated 77%)\n",
            "  adding: content/texts/65.txt (deflated 89%)\n",
            "  adding: content/texts/58.txt (deflated 79%)\n",
            "  adding: content/texts/24.txt (deflated 77%)\n",
            "  adding: content/texts/73.txt (deflated 74%)\n",
            "  adding: content/texts/48.txt (deflated 85%)\n",
            "  adding: content/texts/12.txt (deflated 53%)\n",
            "  adding: content/texts/5.txt (deflated 60%)\n",
            "  adding: content/texts/90.txt (deflated 83%)\n",
            "  adding: content/texts/30.txt (deflated 66%)\n",
            "  adding: content/texts/89.txt (deflated 67%)\n",
            "  adding: content/texts/18.txt (deflated 66%)\n",
            "  adding: content/texts/93.txt (deflated 85%)\n",
            "  adding: content/texts/46.txt (deflated 75%)\n",
            "  adding: content/texts/95.txt (deflated 86%)\n",
            "  adding: content/texts/94.txt (deflated 69%)\n",
            "  adding: content/texts/84.txt (deflated 65%)\n",
            "  adding: content/texts/66.txt (deflated 79%)\n",
            "  adding: content/texts/97.txt (deflated 67%)\n",
            "  adding: content/texts/44.txt (deflated 78%)\n",
            "  adding: content/texts/50.txt (deflated 78%)\n",
            "  adding: content/texts/74.txt (deflated 61%)\n",
            "  adding: content/texts/27.txt (deflated 55%)\n",
            "  adding: content/texts/14.txt (deflated 74%)\n",
            "  adding: content/texts/78.txt (deflated 65%)\n",
            "  adding: content/texts/9.txt (deflated 63%)\n",
            "  adding: content/texts/64.txt (deflated 86%)\n",
            "  adding: content/texts/54.txt (deflated 78%)\n",
            "  adding: content/texts/25.txt (deflated 65%)\n",
            "  adding: content/texts/96.txt (deflated 70%)\n",
            "  adding: content/texts/1.txt (deflated 60%)\n",
            "  adding: content/texts/36.txt (deflated 79%)\n",
            "  adding: content/texts/88.txt (deflated 68%)\n",
            "  adding: content/texts/79.txt (deflated 64%)\n",
            "  adding: content/texts/33.txt (deflated 62%)\n",
            "  adding: content/texts/63.txt (deflated 84%)\n",
            "  adding: content/texts/39.txt (deflated 78%)\n",
            "  adding: content/texts/100.txt (deflated 67%)\n",
            "  adding: content/texts/10.txt (deflated 64%)\n",
            "  adding: content/texts/32.txt (deflated 60%)\n",
            "  adding: content/texts/70.txt (deflated 82%)\n",
            "  adding: content/texts/21.txt (deflated 65%)\n",
            "  adding: content/texts/13.txt (deflated 62%)\n",
            "  adding: content/texts/82.txt (deflated 64%)\n",
            "  adding: content/texts/42.txt (deflated 78%)\n",
            "  adding: content/texts/4.txt (deflated 61%)\n",
            "  adding: content/texts/49.txt (deflated 77%)\n",
            "  adding: content/texts/55.txt (deflated 80%)\n",
            "  adding: content/texts/6.txt (deflated 72%)\n",
            "  adding: content/texts/86.txt (deflated 69%)\n",
            "  adding: content/texts/17.txt (deflated 54%)\n",
            "  adding: content/texts/77.txt (deflated 54%)\n",
            "  adding: content/texts/80.txt (deflated 62%)\n",
            "  adding: content/texts/68.txt (deflated 77%)\n",
            "  adding: content/texts/45.txt (deflated 85%)\n",
            "  adding: content/texts/56.txt (deflated 79%)\n",
            "  adding: content/texts/69.txt (deflated 77%)\n",
            "  adding: content/texts/31.txt (deflated 66%)\n",
            "  adding: content/texts/43.txt (deflated 84%)\n",
            "  adding: content/texts/11.txt (deflated 53%)\n",
            "  adding: content/texts/22.txt (deflated 65%)\n",
            "  adding: content/texts/62.txt (deflated 83%)\n",
            "  adding: content/texts/38.txt (deflated 79%)\n",
            "  adding: content/texts/59.txt (deflated 80%)\n",
            "  adding: content/texts/92.txt (deflated 69%)\n",
            "  adding: content/texts/2.txt (deflated 77%)\n",
            "  adding: content/texts/40.txt (deflated 78%)\n",
            "  adding: content/texts/8.txt (deflated 62%)\n",
            "  adding: content/texts/75.txt (deflated 65%)\n",
            "  adding: content/texts/28.txt (deflated 73%)\n",
            "  adding: content/texts/16.txt (deflated 54%)\n",
            "  adding: content/texts/19.txt (deflated 76%)\n",
            "  adding: content/texts/83.txt (deflated 80%)\n",
            "  adding: content/texts/7.txt (deflated 61%)\n",
            "  adding: content/texts/60.txt (deflated 77%)\n",
            "  adding: content/texts/67.txt (deflated 80%)\n",
            "  adding: content/texts/23.txt (deflated 77%)\n",
            "  adding: content/texts/37.txt (deflated 78%)\n",
            "  adding: content/texts/3.txt (deflated 65%)\n",
            "  adding: content/texts/91.txt (deflated 81%)\n",
            "  adding: content/texts/53.txt (deflated 73%)\n",
            "  adding: content/texts/85.txt (deflated 69%)\n",
            "  adding: content/texts/98.txt (deflated 86%)\n",
            "  adding: content/texts/71.txt (deflated 70%)\n",
            "  adding: content/texts/20.txt (deflated 61%)\n",
            "  adding: content/texts/99.txt (deflated 65%)\n",
            "  adding: content/tokens/ (stored 0%)\n",
            "  adding: content/tokens/tokens_70.txt (deflated 48%)\n",
            "  adding: content/tokens/tokens_82.txt (deflated 52%)\n",
            "  adding: content/tokens/tokens_27.txt (deflated 50%)\n",
            "  adding: content/tokens/tokens_79.txt (deflated 46%)\n",
            "  adding: content/tokens/tokens_16.txt (deflated 46%)\n",
            "  adding: content/tokens/tokens_8.txt (deflated 53%)\n",
            "  adding: content/tokens/tokens_60.txt (deflated 58%)\n",
            "  adding: content/tokens/tokens_50.txt (deflated 59%)\n",
            "  adding: content/tokens/tokens_17.txt (deflated 46%)\n",
            "  adding: content/tokens/tokens_65.txt (deflated 43%)\n",
            "  adding: content/tokens/tokens_88.txt (deflated 59%)\n",
            "  adding: content/tokens/tokens_63.txt (deflated 42%)\n",
            "  adding: content/tokens/tokens_89.txt (deflated 60%)\n",
            "  adding: content/tokens/tokens_32.txt (deflated 54%)\n",
            "  adding: content/tokens/tokens_80.txt (deflated 51%)\n",
            "  adding: content/tokens/tokens_40.txt (deflated 59%)\n",
            "  adding: content/tokens/tokens_23.txt (deflated 54%)\n",
            "  adding: content/tokens/tokens_81.txt (deflated 54%)\n",
            "  adding: content/tokens/tokens_56.txt (deflated 53%)\n",
            "  adding: content/tokens/tokens_36.txt (deflated 55%)\n",
            "  adding: content/tokens/tokens_34.txt (deflated 49%)\n",
            "  adding: content/tokens/tokens_77.txt (deflated 50%)\n",
            "  adding: content/tokens/tokens_64.txt (deflated 44%)\n",
            "  adding: content/tokens/tokens_86.txt (deflated 60%)\n",
            "  adding: content/tokens/tokens_67.txt (deflated 44%)\n",
            "  adding: content/tokens/tokens_44.txt (deflated 54%)\n",
            "  adding: content/tokens/tokens_28.txt (deflated 55%)\n",
            "  adding: content/tokens/tokens_7.txt (deflated 47%)\n",
            "  adding: content/tokens/tokens_62.txt (deflated 39%)\n",
            "  adding: content/tokens/tokens_94.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_46.txt (deflated 53%)\n",
            "  adding: content/tokens/tokens_33.txt (deflated 51%)\n",
            "  adding: content/tokens/tokens_20.txt (deflated 52%)\n",
            "  adding: content/tokens/tokens_54.txt (deflated 59%)\n",
            "  adding: content/tokens/tokens_49.txt (deflated 60%)\n",
            "  adding: content/tokens/tokens_47.txt (deflated 57%)\n",
            "  adding: content/tokens/tokens_92.txt (deflated 60%)\n",
            "  adding: content/tokens/tokens_74.txt (deflated 52%)\n",
            "  adding: content/tokens/tokens_1.txt (deflated 52%)\n",
            "  adding: content/tokens/tokens_52.txt (deflated 57%)\n",
            "  adding: content/tokens/tokens_95.txt (deflated 34%)\n",
            "  adding: content/tokens/tokens_30.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_13.txt (deflated 51%)\n",
            "  adding: content/tokens/tokens_31.txt (deflated 58%)\n",
            "  adding: content/tokens/tokens_14.txt (deflated 49%)\n",
            "  adding: content/tokens/tokens_6.txt (deflated 50%)\n",
            "  adding: content/tokens/tokens_55.txt (deflated 55%)\n",
            "  adding: content/tokens/tokens_98.txt (deflated 35%)\n",
            "  adding: content/tokens/tokens_41.txt (deflated 41%)\n",
            "  adding: content/tokens/tokens_45.txt (deflated 40%)\n",
            "  adding: content/tokens/tokens_43.txt (deflated 40%)\n",
            "  adding: content/tokens/tokens_90.txt (deflated 34%)\n",
            "  adding: content/tokens/tokens_71.txt (deflated 53%)\n",
            "  adding: content/tokens/tokens_11.txt (deflated 52%)\n",
            "  adding: content/tokens/tokens_4.txt (deflated 52%)\n",
            "  adding: content/tokens/tokens_97.txt (deflated 61%)\n",
            "  adding: content/tokens/tokens_57.txt (deflated 57%)\n",
            "  adding: content/tokens/tokens_3.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_19.txt (deflated 47%)\n",
            "  adding: content/tokens/tokens_75.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_42.txt (deflated 57%)\n",
            "  adding: content/tokens/tokens_15.txt (deflated 46%)\n",
            "  adding: content/tokens/tokens_78.txt (deflated 54%)\n",
            "  adding: content/tokens/tokens_84.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_53.txt (deflated 55%)\n",
            "  adding: content/tokens/tokens_58.txt (deflated 58%)\n",
            "  adding: content/tokens/tokens_69.txt (deflated 41%)\n",
            "  adding: content/tokens/tokens_72.txt (deflated 51%)\n",
            "  adding: content/tokens/tokens_5.txt (deflated 53%)\n",
            "  adding: content/tokens/tokens_9.txt (deflated 50%)\n",
            "  adding: content/tokens/tokens_59.txt (deflated 58%)\n",
            "  adding: content/tokens/tokens_37.txt (deflated 58%)\n",
            "  adding: content/tokens/tokens_76.txt (deflated 51%)\n",
            "  adding: content/tokens/tokens_35.txt (deflated 54%)\n",
            "  adding: content/tokens/tokens_38.txt (deflated 58%)\n",
            "  adding: content/tokens/tokens_96.txt (deflated 55%)\n",
            "  adding: content/tokens/tokens_61.txt (deflated 55%)\n",
            "  adding: content/tokens/tokens_48.txt (deflated 40%)\n",
            "  adding: content/tokens/tokens_12.txt (deflated 52%)\n",
            "  adding: content/tokens/tokens_66.txt (deflated 55%)\n",
            "  adding: content/tokens/tokens_91.txt (deflated 35%)\n",
            "  adding: content/tokens/tokens_22.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_73.txt (deflated 51%)\n",
            "  adding: content/tokens/tokens_51.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_10.txt (deflated 51%)\n",
            "  adding: content/tokens/tokens_39.txt (deflated 58%)\n",
            "  adding: content/tokens/tokens_24.txt (deflated 54%)\n",
            "  adding: content/tokens/tokens_100.txt (deflated 63%)\n",
            "  adding: content/tokens/tokens_87.txt (deflated 60%)\n",
            "  adding: content/tokens/tokens_83.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_93.txt (deflated 36%)\n",
            "  adding: content/tokens/tokens_25.txt (deflated 56%)\n",
            "  adding: content/tokens/tokens_99.txt (deflated 62%)\n",
            "  adding: content/tokens/tokens_18.txt (deflated 46%)\n",
            "  adding: content/tokens/tokens_26.txt (deflated 49%)\n",
            "  adding: content/tokens/tokens_68.txt (deflated 42%)\n",
            "  adding: content/tokens/tokens_21.txt (deflated 57%)\n",
            "  adding: content/tokens/tokens_2.txt (deflated 54%)\n",
            "  adding: content/tokens/tokens_29.txt (deflated 49%)\n",
            "  adding: content/tokens/tokens_85.txt (deflated 57%)\n",
            "  adding: content/lemmas/ (stored 0%)\n",
            "  adding: content/lemmas/lemmas_60.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_45.txt (deflated 67%)\n",
            "  adding: content/lemmas/lemmas_44.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_31.txt (deflated 77%)\n",
            "  adding: content/lemmas/lemmas_21.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_75.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_80.txt (deflated 73%)\n",
            "  adding: content/lemmas/lemmas_42.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_48.txt (deflated 68%)\n",
            "  adding: content/lemmas/lemmas_22.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_69.txt (deflated 68%)\n",
            "  adding: content/lemmas/lemmas_46.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_98.txt (deflated 65%)\n",
            "  adding: content/lemmas/lemmas_18.txt (deflated 71%)\n",
            "  adding: content/lemmas/lemmas_59.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_62.txt (deflated 68%)\n",
            "  adding: content/lemmas/lemmas_37.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_33.txt (deflated 73%)\n",
            "  adding: content/lemmas/lemmas_93.txt (deflated 66%)\n",
            "  adding: content/lemmas/lemmas_86.txt (deflated 78%)\n",
            "  adding: content/lemmas/lemmas_50.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_99.txt (deflated 78%)\n",
            "  adding: content/lemmas/lemmas_96.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_24.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_87.txt (deflated 77%)\n",
            "  adding: content/lemmas/lemmas_79.txt (deflated 71%)\n",
            "  adding: content/lemmas/lemmas_89.txt (deflated 78%)\n",
            "  adding: content/lemmas/lemmas_16.txt (deflated 72%)\n",
            "  adding: content/lemmas/lemmas_65.txt (deflated 69%)\n",
            "  adding: content/lemmas/lemmas_57.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_100.txt (deflated 79%)\n",
            "  adding: content/lemmas/lemmas_84.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_15.txt (deflated 72%)\n",
            "  adding: content/lemmas/lemmas_81.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_49.txt (deflated 77%)\n",
            "  adding: content/lemmas/lemmas_54.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_13.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_77.txt (deflated 73%)\n",
            "  adding: content/lemmas/lemmas_70.txt (deflated 71%)\n",
            "  adding: content/lemmas/lemmas_34.txt (deflated 73%)\n",
            "  adding: content/lemmas/lemmas_76.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_43.txt (deflated 68%)\n",
            "  adding: content/lemmas/lemmas_9.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_38.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_10.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_39.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_55.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_78.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_23.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_11.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_7.txt (deflated 73%)\n",
            "  adding: content/lemmas/lemmas_3.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_28.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_91.txt (deflated 65%)\n",
            "  adding: content/lemmas/lemmas_35.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_88.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_61.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_4.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_56.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_40.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_64.txt (deflated 70%)\n",
            "  adding: content/lemmas/lemmas_26.txt (deflated 73%)\n",
            "  adding: content/lemmas/lemmas_17.txt (deflated 72%)\n",
            "  adding: content/lemmas/lemmas_63.txt (deflated 69%)\n",
            "  adding: content/lemmas/lemmas_20.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_95.txt (deflated 65%)\n",
            "  adding: content/lemmas/lemmas_1.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_72.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_27.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_71.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_90.txt (deflated 65%)\n",
            "  adding: content/lemmas/lemmas_92.txt (deflated 77%)\n",
            "  adding: content/lemmas/lemmas_97.txt (deflated 77%)\n",
            "  adding: content/lemmas/lemmas_83.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_29.txt (deflated 72%)\n",
            "  adding: content/lemmas/lemmas_25.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_68.txt (deflated 69%)\n",
            "  adding: content/lemmas/lemmas_47.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_36.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_8.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_30.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_67.txt (deflated 70%)\n",
            "  adding: content/lemmas/lemmas_51.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_12.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_82.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_6.txt (deflated 73%)\n",
            "  adding: content/lemmas/lemmas_73.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_85.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_74.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_52.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_5.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_58.txt (deflated 76%)\n",
            "  adding: content/lemmas/lemmas_53.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_66.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_41.txt (deflated 68%)\n",
            "  adding: content/lemmas/lemmas_19.txt (deflated 71%)\n",
            "  adding: content/lemmas/lemmas_32.txt (deflated 75%)\n",
            "  adding: content/lemmas/lemmas_14.txt (deflated 73%)\n",
            "  adding: content/lemmas/lemmas_2.txt (deflated 74%)\n",
            "  adding: content/lemmas/lemmas_94.txt (deflated 76%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/lemmas.zip\")\n",
        "files.download(\"/content/tokens.zip\")\n",
        "files.download(\"/content/texts.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uz3l2AhLl6H8",
        "outputId": "a235b286-7442-4e1f-e628-a5e07d2ff61f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_471839a7-f1c6-4afd-9f89-98170e563d1d\", \"lemmas.zip\", 472873)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0d3d2759-57a5-4489-b068-a748cee64669\", \"tokens.zip\", 295429)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f2f278c3-fb92-4629-a974-c84562dcf157\", \"texts.zip\", 1091716)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 4"
      ],
      "metadata": {
        "id": "Y-2qqehr9t3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unicodecsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXgDflv998GC",
        "outputId": "487fbb67-0990-4646-c3bf-01a837bc0c5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unicodecsv\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10746 sha256=26396b972cf2546e8a1c0a132f5b57e3367e0e62f76665943342b3eeba92ed49\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv\n",
            "Successfully installed unicodecsv-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install methods"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJpVPRuc-ETd",
        "outputId": "e41d64f7-98a0-416a-e3fa-2b4cb5c9326d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement methods (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for methods\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pymorphy3\n",
        "import io\n",
        "import re\n",
        "from bisect import bisect_left\n",
        "import unicodecsv as csv\n",
        "import pymorphy3\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "from bisect import bisect_left\n",
        "import unicodecsv as csv\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "from collections import Counter\n",
        "\n",
        "files = {}\n",
        "\n",
        "def search(a, x, lo=0, hi=None):\n",
        "    if hi is None: hi = len(a)\n",
        "    pos = bisect_left(a, x, lo, hi)\n",
        "    return pos if pos != hi and a[pos] == x else -1\n",
        "\n",
        "def find(target, L):\n",
        "    if L in target:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def count_words(word, file):\n",
        "    l = word.lower()\n",
        "    if files.get(file) is not None:\n",
        "        words = files.get(file)[\"words\"]\n",
        "        return find(words, l)\n",
        "\n",
        "\n",
        "def check_lemmas(lemma, file):\n",
        "    l = lemma.lower()\n",
        "    if files.get(file) is not None:\n",
        "        lemmas = files.get(file)[\"lemmas\"]\n",
        "\n",
        "        return find(lemmas, l)\n",
        "\n",
        "\n",
        "def check_word_in_files(word, files):\n",
        "    documents = []\n",
        "    for f in files:\n",
        "        if count_words(word, f) is True:\n",
        "            documents.append(f)\n",
        "    return documents\n",
        "\n",
        "\n",
        "def check_lemma_in_files(lemma, files):\n",
        "    documents = []\n",
        "    for f in files:\n",
        "        if check_lemmas(lemma, f) is True:\n",
        "            documents.append(f)\n",
        "    return documents\n",
        "\n",
        "\n",
        "def lemmatize(words, tokenized_map):\n",
        "    for word in words:\n",
        "        lowered = word.lower()\n",
        "        if pymorphy3.MorphAnalyzer().parse(word)[0].tag.POS(lowered) not in {'INTJ', 'PRCL', 'CONJ', 'PREP'}:\n",
        "            p = morph.parse(word)[0].normal_form\n",
        "            arr = tokenized_map.get(p)\n",
        "            if arr is None:\n",
        "                new_arr = [lowered]\n",
        "                tokenized_map[p] = new_arr\n",
        "            else:\n",
        "                arr.append(lowered)\n",
        "\n",
        "\n",
        "def get_clear_words_from_file(file):\n",
        "    print(file)\n",
        "    with io.open(file, mode='r', encoding=\"utf-8\") as f:\n",
        "        return f.read().split('\\n')\n",
        "\n",
        "\n",
        "\n",
        "def get_files_from_path(path):\n",
        "    arr = os.listdir(path)\n",
        "    return arr\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "from collections import Counter\n",
        "\n",
        "all_files = {}\n",
        "\n",
        "def get_lemma_file_by_token_file(file):\n",
        "    file_index = int(re.search(r'\\d+', file).group())\n",
        "    found = None\n",
        "    for f in lemma_files:\n",
        "        if re.search(r'\\d+', f) is not None:\n",
        "            f_index = int(re.search(r'\\d+', f).group())\n",
        "            if f_index == file_index:\n",
        "                found = f\n",
        "                break\n",
        "    return found\n",
        "\n",
        "\n",
        "files = get_files_from_path(\"/content/tokens\")\n",
        "lemma_files = get_files_from_path(\"/content/lemmas\")\n",
        "\n",
        "for f in files:\n",
        "        tokenized_map = {}\n",
        "\n",
        "        words = get_clear_words_from_file(f)\n",
        "\n",
        "        lemma_file = get_lemma_file_by_token_file(f)\n",
        "        lemmas = get_clear_words_from_file(lemma_file)\n",
        "        all_files[f] = {}\n",
        "\n",
        "\n",
        "for f in files:\n",
        "    print(f)\n",
        "    tokenized_map = {}\n",
        "    try:\n",
        "      words = all_files[f][\"words\"]\n",
        "      lemmas = all_files[f][\"lemmas\"]\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    lemmatize(words, tokenized_map)\n",
        "\n",
        "    total_token_count = len(words)\n",
        "    c = Counter(words)\n",
        "    uniq_token = c.keys()\n",
        "\n",
        "    with io.open(\"tokensTf/tf\" + f + \".csv\", 'w', encoding=\"utf-8\") as tff:\n",
        "        t_str = \"TOKEN;TF;IDF;TF*IDF\"\n",
        "        tff.write(f\"{t_str}\\n\")\n",
        "        for t in uniq_token:\n",
        "            tf = round((c[t] / total_token_count), 6)\n",
        "            document_count_for_t = check_word_in_files(t, files)\n",
        "            idf = round(math.log2(len(files) / len(document_count_for_t)), 6)\n",
        "            tf_idf = round(tf * idf, 6)\n",
        "            t_str = f\"{t};{tf};{idf};{tf_idf}\"\n",
        "            tff.write(f\"{t_str}\\n\")\n",
        "\n",
        "    lemmas_count = len(tokenized_map.keys())\n",
        "\n",
        "    with io.open(\"lemmaTf/lemma\" + f + \".csv\", 'w', encoding=\"utf-8\") as tff:\n",
        "        t_str = \"LEMMA;TF;IDF;TF*IDF\"\n",
        "        tff.write(f\"{t_str}\\n\")\n",
        "        for l in lemmas:\n",
        "            tf = round(len(tokenized_map[l]) / lemmas_count,6)\n",
        "            document_count_for_l = check_lemma_in_files(l, files)\n",
        "            idf = round(math.log2(len(files) / len(document_count_for_l)),6)\n",
        "            tf_idf = round(tf * idf, 6)\n",
        "            t_str = f\"{l};{tf};{idf};{tf_idf}\"\n",
        "            tff.write(f\"{t_str}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K_0mvVD9xWE",
        "outputId": "36e154b8-f90d-4131-90fe-1e443c02b261"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_70.txt\n",
            "lemmas_70.txt\n",
            "tokens_82.txt\n",
            "lemmas_82.txt\n",
            "tokens_27.txt\n",
            "lemmas_27.txt\n",
            "tokens_79.txt\n",
            "lemmas_79.txt\n",
            "tokens_16.txt\n",
            "lemmas_16.txt\n",
            "tokens_8.txt\n",
            "lemmas_8.txt\n",
            "tokens_60.txt\n",
            "lemmas_60.txt\n",
            "tokens_50.txt\n",
            "lemmas_50.txt\n",
            "tokens_17.txt\n",
            "lemmas_17.txt\n",
            "tokens_65.txt\n",
            "lemmas_65.txt\n",
            "tokens_88.txt\n",
            "lemmas_88.txt\n",
            "tokens_63.txt\n",
            "lemmas_63.txt\n",
            "tokens_89.txt\n",
            "lemmas_89.txt\n",
            "tokens_32.txt\n",
            "lemmas_32.txt\n",
            "tokens_80.txt\n",
            "lemmas_80.txt\n",
            "tokens_40.txt\n",
            "lemmas_40.txt\n",
            "tokens_23.txt\n",
            "lemmas_23.txt\n",
            "tokens_81.txt\n",
            "lemmas_81.txt\n",
            "tokens_56.txt\n",
            "lemmas_56.txt\n",
            "tokens_36.txt\n",
            "lemmas_36.txt\n",
            "tokens_34.txt\n",
            "lemmas_34.txt\n",
            "tokens_77.txt\n",
            "lemmas_77.txt\n",
            "tokens_64.txt\n",
            "lemmas_64.txt\n",
            "tokens_86.txt\n",
            "lemmas_86.txt\n",
            "tokens_67.txt\n",
            "lemmas_67.txt\n",
            "tokens_44.txt\n",
            "lemmas_44.txt\n",
            "tokens_28.txt\n",
            "lemmas_28.txt\n",
            "tokens_7.txt\n",
            "lemmas_7.txt\n",
            "tokens_62.txt\n",
            "lemmas_62.txt\n",
            "tokens_94.txt\n",
            "lemmas_94.txt\n",
            "tokens_46.txt\n",
            "lemmas_46.txt\n",
            "tokens_33.txt\n",
            "lemmas_33.txt\n",
            "tokens_20.txt\n",
            "lemmas_20.txt\n",
            "tokens_54.txt\n",
            "lemmas_54.txt\n",
            "tokens_49.txt\n",
            "lemmas_49.txt\n",
            "tokens_47.txt\n",
            "lemmas_47.txt\n",
            "tokens_92.txt\n",
            "lemmas_92.txt\n",
            "tokens_74.txt\n",
            "lemmas_74.txt\n",
            "tokens_1.txt\n",
            "lemmas_1.txt\n",
            "tokens_52.txt\n",
            "lemmas_52.txt\n",
            "tokens_95.txt\n",
            "lemmas_95.txt\n",
            "tokens_30.txt\n",
            "lemmas_30.txt\n",
            "tokens_13.txt\n",
            "lemmas_13.txt\n",
            "tokens_31.txt\n",
            "lemmas_31.txt\n",
            "tokens_14.txt\n",
            "lemmas_14.txt\n",
            "tokens_6.txt\n",
            "lemmas_6.txt\n",
            "tokens_55.txt\n",
            "lemmas_55.txt\n",
            "tokens_98.txt\n",
            "lemmas_98.txt\n",
            "tokens_41.txt\n",
            "lemmas_41.txt\n",
            "tokens_45.txt\n",
            "lemmas_45.txt\n",
            "tokens_43.txt\n",
            "lemmas_43.txt\n",
            "tokens_90.txt\n",
            "lemmas_90.txt\n",
            "tokens_71.txt\n",
            "lemmas_71.txt\n",
            "tokens_11.txt\n",
            "lemmas_11.txt\n",
            "tokens_4.txt\n",
            "lemmas_4.txt\n",
            "tokens_97.txt\n",
            "lemmas_97.txt\n",
            "tokens_57.txt\n",
            "lemmas_57.txt\n",
            "tokens_3.txt\n",
            "lemmas_3.txt\n",
            "tokens_19.txt\n",
            "lemmas_19.txt\n",
            "tokens_75.txt\n",
            "lemmas_75.txt\n",
            "tokens_42.txt\n",
            "lemmas_42.txt\n",
            "tokens_15.txt\n",
            "lemmas_15.txt\n",
            "tokens_78.txt\n",
            "lemmas_78.txt\n",
            "tokens_84.txt\n",
            "lemmas_84.txt\n",
            "tokens_53.txt\n",
            "lemmas_53.txt\n",
            "tokens_58.txt\n",
            "lemmas_58.txt\n",
            "tokens_69.txt\n",
            "lemmas_69.txt\n",
            "tokens_72.txt\n",
            "lemmas_72.txt\n",
            "tokens_5.txt\n",
            "lemmas_5.txt\n",
            "tokens_9.txt\n",
            "lemmas_9.txt\n",
            "tokens_59.txt\n",
            "lemmas_59.txt\n",
            "tokens_37.txt\n",
            "lemmas_37.txt\n",
            "tokens_76.txt\n",
            "lemmas_76.txt\n",
            "tokens_35.txt\n",
            "lemmas_35.txt\n",
            "tokens_38.txt\n",
            "lemmas_38.txt\n",
            "tokens_96.txt\n",
            "lemmas_96.txt\n",
            "tokens_61.txt\n",
            "lemmas_61.txt\n",
            "tokens_48.txt\n",
            "lemmas_48.txt\n",
            "tokens_12.txt\n",
            "lemmas_12.txt\n",
            "tokens_66.txt\n",
            "lemmas_66.txt\n",
            "tokens_91.txt\n",
            "lemmas_91.txt\n",
            "tokens_22.txt\n",
            "lemmas_22.txt\n",
            "tokens_73.txt\n",
            "lemmas_73.txt\n",
            "tokens_51.txt\n",
            "lemmas_51.txt\n",
            "tokens_10.txt\n",
            "lemmas_10.txt\n",
            "tokens_39.txt\n",
            "lemmas_39.txt\n",
            "tokens_24.txt\n",
            "lemmas_24.txt\n",
            "tokens_100.txt\n",
            "lemmas_100.txt\n",
            "tokens_87.txt\n",
            "lemmas_87.txt\n",
            "tokens_83.txt\n",
            "lemmas_83.txt\n",
            "tokens_93.txt\n",
            "lemmas_93.txt\n",
            "tokens_25.txt\n",
            "lemmas_25.txt\n",
            "tokens_99.txt\n",
            "lemmas_99.txt\n",
            "tokens_18.txt\n",
            "lemmas_18.txt\n",
            "tokens_26.txt\n",
            "lemmas_26.txt\n",
            "tokens_68.txt\n",
            "lemmas_68.txt\n",
            "tokens_21.txt\n",
            "lemmas_21.txt\n",
            "tokens_2.txt\n",
            "lemmas_2.txt\n",
            "tokens_29.txt\n",
            "lemmas_29.txt\n",
            "tokens_85.txt\n",
            "lemmas_85.txt\n",
            "tokens_70.txt\n",
            "tokens_82.txt\n",
            "tokens_27.txt\n",
            "tokens_79.txt\n",
            "tokens_16.txt\n",
            "tokens_8.txt\n",
            "tokens_60.txt\n",
            "tokens_50.txt\n",
            "tokens_17.txt\n",
            "tokens_65.txt\n",
            "tokens_88.txt\n",
            "tokens_63.txt\n",
            "tokens_89.txt\n",
            "tokens_32.txt\n",
            "tokens_80.txt\n",
            "tokens_40.txt\n",
            "tokens_23.txt\n",
            "tokens_81.txt\n",
            "tokens_56.txt\n",
            "tokens_36.txt\n",
            "tokens_34.txt\n",
            "tokens_77.txt\n",
            "tokens_64.txt\n",
            "tokens_86.txt\n",
            "tokens_67.txt\n",
            "tokens_44.txt\n",
            "tokens_28.txt\n",
            "tokens_7.txt\n",
            "tokens_62.txt\n",
            "tokens_94.txt\n",
            "tokens_46.txt\n",
            "tokens_33.txt\n",
            "tokens_20.txt\n",
            "tokens_54.txt\n",
            "tokens_49.txt\n",
            "tokens_47.txt\n",
            "tokens_92.txt\n",
            "tokens_74.txt\n",
            "tokens_1.txt\n",
            "tokens_52.txt\n",
            "tokens_95.txt\n",
            "tokens_30.txt\n",
            "tokens_13.txt\n",
            "tokens_31.txt\n",
            "tokens_14.txt\n",
            "tokens_6.txt\n",
            "tokens_55.txt\n",
            "tokens_98.txt\n",
            "tokens_41.txt\n",
            "tokens_45.txt\n",
            "tokens_43.txt\n",
            "tokens_90.txt\n",
            "tokens_71.txt\n",
            "tokens_11.txt\n",
            "tokens_4.txt\n",
            "tokens_97.txt\n",
            "tokens_57.txt\n",
            "tokens_3.txt\n",
            "tokens_19.txt\n",
            "tokens_75.txt\n",
            "tokens_42.txt\n",
            "tokens_15.txt\n",
            "tokens_78.txt\n",
            "tokens_84.txt\n",
            "tokens_53.txt\n",
            "tokens_58.txt\n",
            "tokens_69.txt\n",
            "tokens_72.txt\n",
            "tokens_5.txt\n",
            "tokens_9.txt\n",
            "tokens_59.txt\n",
            "tokens_37.txt\n",
            "tokens_76.txt\n",
            "tokens_35.txt\n",
            "tokens_38.txt\n",
            "tokens_96.txt\n",
            "tokens_61.txt\n",
            "tokens_48.txt\n",
            "tokens_12.txt\n",
            "tokens_66.txt\n",
            "tokens_91.txt\n",
            "tokens_22.txt\n",
            "tokens_73.txt\n",
            "tokens_51.txt\n",
            "tokens_10.txt\n",
            "tokens_39.txt\n",
            "tokens_24.txt\n",
            "tokens_100.txt\n",
            "tokens_87.txt\n",
            "tokens_83.txt\n",
            "tokens_93.txt\n",
            "tokens_25.txt\n",
            "tokens_99.txt\n",
            "tokens_18.txt\n",
            "tokens_26.txt\n",
            "tokens_68.txt\n",
            "tokens_21.txt\n",
            "tokens_2.txt\n",
            "tokens_29.txt\n",
            "tokens_85.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf `find -type d -name .ipynb_checkpoints`"
      ],
      "metadata": {
        "id": "Zp19ml7J3RJy"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}